{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f572375",
   "metadata": {},
   "source": [
    "<img src=\"logounav.png\" width=\"150\" img style=\"float: right;\"> \n",
    "\n",
    "**Analysis of the influence of hyperparameters in a neural network for an analysis of glassdoor bids for data scientists.**<br>\n",
    "Author: Lucía Colín Cosano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061ff081",
   "metadata": {},
   "source": [
    "The following notebook develops the use of neural networks. For this purpose, a database related to job offers for data scientist that have been published in Glassdoor has been selected.\n",
    "\n",
    "The points treated have been the following ones:\n",
    "- Definition of the problem and description of the variables.\n",
    "- Data loading.\n",
    "- Exploratory analysis of the data.\n",
    "- Transformation of the variables.\n",
    "- Visualization of distributions, identification of outliers and correlation matrix.\n",
    "- Modeling and analysis of results. Modification of hyperparameters and implementation of different architectures.\n",
    "- Conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "898b2a19",
   "metadata": {},
   "source": [
    "### DEFINITION OF THE PROBLEM AND DESCRIPTION OF THE VARIABLES."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87bbbe4",
   "metadata": {},
   "source": [
    "**Glassdoor** is a Web site that provides information about companies, jobs and salaries. It allows users to search for and rate companies, read reviews written by current and former employees, and compare salaries and benefits in different industries and geographic regions. It also has a job interview section, where users can read about the experience of other candidates in the company's selection process. \n",
    "\n",
    "Glassdoor is used by professional job seekers to learn about working conditions and company cultures, and by companies to attract and retain talent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf4873e",
   "metadata": {},
   "source": [
    "The available dataset has the variables described below:\n",
    "- **Index:** contains the number of the observation.\n",
    "- **Job title:** refers to the position being offered.\n",
    "- **Salary estimate:** salary range of the position expressed in dollars.\n",
    "- **Job description:** description of the activity to be carried out.\n",
    "- **Rating:** rating that the job offer has received.\n",
    "- **Company name:** Company name.\n",
    "- **Location:** Location of the company's offices where the activity is to be carried out.\n",
    "- **Headquarters:** where the boss to whom you report is located.\n",
    "- **Size:** size of the company. This is expressed in different ranges, not giving an exact amount.\n",
    "- **Founded:** Year in which the company was founded.\n",
    "- **Type of ownership:** what type of company it is (public, private...).\n",
    "- **Industry:** industry in which you work.\n",
    "- **Sector:** field in which the data scientist will work.\n",
    "- **Revenue:** total revenue of the company.\n",
    "- **Competitors**: list of direct competitors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df19ca0",
   "metadata": {},
   "source": [
    "Knowing the salary range in an industry or for a specific job is **important when making decisions** about a job offer as it allows to set expectations, negotiate salaries, compare job offers and make a proper financial planning. The goal is to be able to predict the **average salary** using neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cdb80e",
   "metadata": {},
   "source": [
    "### DATA LOADING."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a445884",
   "metadata": {},
   "source": [
    "First, the libraries needed to solve the problem are imported and the database is read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0187a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from skimpy import skim\n",
    "import ydata_profiling\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score,recall_score,f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import shap\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0691c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('DS_jobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9883a0",
   "metadata": {},
   "source": [
    "### EXPLORATIVE DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b11319",
   "metadata": {},
   "source": [
    "Once the dataset has been read, an exportable analysis of the samples to be analyzed will be performed. To do this, the **skim** and **pd.profiling** functions will be used first, as they allow a quick view of the database. The existence of null values and the existence of columns with unique values are checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d755f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#skim(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e116ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#report = ydata_profiling.ProfileReport(df)\n",
    "#report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad3b007",
   "metadata": {},
   "source": [
    "The following information is obtained from the report:\n",
    "- **Data shape:** there are 15 variables and 672 observations. \n",
    "- There are no **duplicate observations**.\n",
    "- There are no columns with a **single value**.\n",
    "- The columns with **few values** are those that refer to **categories**, so it makes sense.\n",
    "- It is a **complete database in terms of null values** although it requires numerous transformations.\n",
    "- The **anomalous values** in the variables are identified with a -1, which, as they occur in cateogric variables, a category called \"Unknown\" has been defined.\n",
    "\n",
    "Although the information offered is greater, this will be done again once the variables have been transformed as it is considered to be more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03222422",
   "metadata": {},
   "source": [
    "Secondly, variables that will be irrelevant for the modeling are eliminated, such as **Job Title**, since the study is carried out on a single profession, **Job Description**, due to the difficulty of processing such a large number of words, **Competitors**, since these are very numerous and different between companies and **index**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c0350e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before\", df.shape)\n",
    "#df = df.drop(['index'], axis=1)\n",
    "df = df.drop(['Job Title'], axis=1)\n",
    "df = df.drop(['Job Description'], axis=1)\n",
    "df = df.drop(['Competitors'], axis=1)\n",
    "print(\"After\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47708206",
   "metadata": {},
   "source": [
    "In addition, from the information obtained from the **pandas profiling** it has been verified that the repetitions of the company name (variable **Company Name**) are not high, so that transforming the names would be very costly, since it would increase the dimensionality of the problem when this information can be reflected in other variables such as the sector in which it operates, along with the year of foundation, the number of employees and the total revenue.\n",
    "\n",
    "For this reason, it was decided to dispense with the **Company Name** variable for this particular study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2eb6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Company Name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97859bb",
   "metadata": {},
   "source": [
    "### FEATURE TRANSFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a92cc3d",
   "metadata": {},
   "source": [
    "The dataset needs numerous transformations in order to understand it better and for this purpose the variables that are expressed in the form of ranges will be decomposed.\n",
    "- **Salary Estimate**\n",
    "- **Size**\n",
    "- **Revenue** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9035fb",
   "metadata": {},
   "source": [
    "The column is divided into two values, minimum and maximum, and intervals consistent with the magnitude of each variable are defined. In order to be able to define this magnitude with criterion, these variables are represented graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49910b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = df[\"Size\"].str.split(expand=True)\n",
    "size.columns = ['Size_min', 'str1','Size_max','str2']\n",
    "df = pd.concat([df, size], axis=1)\n",
    "\n",
    "df = df.drop(['Size'], axis=1)\n",
    "df = df.drop(['str1'], axis=1)\n",
    "df = df.drop(['str2'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01d3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Size_min', data=df,palette='crest',order=['Unknown','-1','1','51','201','501','1001','5001','10000+'])\n",
    "plt.xlabel('Número mínimo de empleados')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Número mínimo de empleados de las empresas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728b6252",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Size_max', data=df,palette='crest',order=['50','200','500','1000','5000','10000'])\n",
    "plt.xlabel('Número máximo de empleados de las empresas')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Número máximo de empleados de las empresas')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d9e20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_companies(num_empleados):\n",
    "    if num_empleados < 500:\n",
    "        return \"Small business\"\n",
    "    elif num_empleados <1000:\n",
    "        return \"Medium business\"\n",
    "    elif num_empleados >1000:\n",
    "        return \"Large business\"\n",
    "    else:\n",
    "        return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1406527e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Size_max'] = df['Size_max'].fillna(0)\n",
    "df['Size_max'] = df['Size_max'].astype(int)\n",
    "df['Size'] = df['Size_max'].apply(classify_companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f065ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Size', data=df,palette='crest', order=[\"Unknown\",\"Small business\",'Medium business','Large business'])\n",
    "\n",
    "plt.xlabel('')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Tipo de empresa analizada')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f24037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ df['Size'] =='Unknown', 'Size'] = 0\n",
    "df.loc[df['Size'] =='Small business', 'Size'] = 1\n",
    "df.loc[df['Size'] =='Medium business', 'Size']   = 2\n",
    "df.loc[ df['Size'] =='Large business', 'Size'] = 3\n",
    "df['Size'] = df['Size'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff902d03",
   "metadata": {},
   "source": [
    "The column referring to **Type of ownership** is given with an irregular format depending on the type, since when the company is public or private it is accompanied by a hyphen and the word company. The column is divided in such a way that only the type of company is obtained. The different existing categories are analyzed and grouped in such a way as to reduce the number of categories to 5**. This grouping is done in order to be able to generalize better since otherwise the number of observations of each type is too few. Therefore, more general adjectives are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025cfd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "owner = df[\"Type of ownership\"].str.split('-',expand=True)\n",
    "owner.columns = ['Type_ownership1', 'Type_ownership2']\n",
    "df = pd.concat([df, owner], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0122c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type_ownership'] = np.where(df['Type_ownership1'] != 'Company ', df['Type_ownership1'], df['Type_ownership2'])\n",
    "df = df.drop(['Type_ownership1'], axis=1)\n",
    "df = df.drop(['Type_ownership2'], axis=1)\n",
    "df['Type_ownership'] = df['Type_ownership'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1612394",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type_ownership']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b4a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = df['Type_ownership'].value_counts()\n",
    "tabla = tabla.reset_index()\n",
    "tabla.columns = ['Type of ownership', 'Frecuencia']\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f911403",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Type_ownership'] = df['Type_ownership'].replace(['', 'Other Organization','Unknown'], 'Unknown')\n",
    "df['Type_ownership'] = df['Type_ownership'].replace(['Self','Private Practice / Firm','Contract'], ' Private')\n",
    "df['Type_ownership'] = df['Type_ownership'].replace(['College / University','Hospital','Government'], ' Public')\n",
    "df['Type_ownership'] = df['Type_ownership'].replace(['Subsidiary or Business Segment'], 'Business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3414fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = df['Type_ownership'].value_counts()\n",
    "tabla = tabla.reset_index()\n",
    "tabla.columns = ['Type of ownership', 'Frecuencia']\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5e8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Type_ownership', data=df,palette='crest')\n",
    "plt.xlabel('Número máximo de empleados de las empresas')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b610276",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Size_min'] = pd.to_numeric(df['Size_min'], errors='coerce')\n",
    "df['Size_max'] = pd.to_numeric(df['Size_max'], errors='coerce')\n",
    "\n",
    "df_publicvsprivate = df[df['Type_ownership'].isin([' Public', ' Private'])]\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(12,5))\n",
    "\n",
    "sns.violinplot(x='Type_ownership', y='Size_min', data=df_publicvsprivate,palette='crest',ax=axs[0])\n",
    "sns.violinplot(x='Type_ownership', y='Size_max', data=df_publicvsprivate,palette='crest',ax=axs[1])\n",
    "plt.ylabel('Size_max')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870783f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivot_table = df.pivot_table(index='Type_ownership', columns='Size', values='Rating', aggfunc='mean')\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8f5d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ df['Type_ownership'] == 'Unknown', 'Type_ownership'] = 0\n",
    "df.loc[df['Type_ownership'] == 'Business', 'Type_ownership'] = 1\n",
    "df.loc[df['Type_ownership'] == 'Nonprofit Organization', 'Type_ownership']   = 2\n",
    "df.loc[ df['Type_ownership'] == ' Public', 'Type_ownership'] = 3\n",
    "df.loc[ df['Type_ownership'] == ' Private','Type_ownership'] = 4\n",
    "df['Type_ownership'] = df['Type_ownership'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b198973",
   "metadata": {},
   "source": [
    "As for the **longevity** of the company, this is expressed according to the year of foundation. We believe it is convenient to transform this variable to age ranges. For this purpose, the datetime function is used, which compares each time the current year is run with the year of foundation. Five segments are defined, which do not comprise the same number of years but try to ensure that each segment is composed of a similar number of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e219cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def def_seniority(founded):\n",
    "    # Obtener el año actual\n",
    "    year_actual = datetime.now().year\n",
    "\n",
    "    # Calcular la antigüedad de la empresa\n",
    "    company_tenure = year_actual - founded\n",
    "\n",
    "    # Definir los rangos de antigüedad\n",
    "    if founded == -1:\n",
    "        return \"Unknown\"\n",
    "    elif company_tenure < 15:\n",
    "        return \"Menos de 15 años\"\n",
    "    elif company_tenure < 25:\n",
    "        return \"15-25 años\"\n",
    "    elif company_tenure < 45:\n",
    "        return \"25-45 años\"\n",
    "    else:\n",
    "        return \"45 años o más\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['company_tenure'] = df['Founded'].apply(def_seniority)\n",
    "\n",
    "sns.countplot(x='company_tenure', data=df, palette='crest')\n",
    "plt.xlabel('Rango de antigüedad')\n",
    "plt.ylabel('Número de empresas')\n",
    "plt.title('Antigüedad de las empresas')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "sns.despine(left=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590b58ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ df['company_tenure'] == 'Unknown', 'company_tenure'] = 0\n",
    "df.loc[df['company_tenure'] == 'Menos de 15 años', 'company_tenure'] = 1\n",
    "df.loc[df['company_tenure'] == '15-25 años', 'company_tenure']   = 2\n",
    "df.loc[ df['company_tenure'] == '25-45 años', 'company_tenure'] = 3\n",
    "df.loc[ df['company_tenure'] == '45 años o más','company_tenure'] = 4\n",
    "df['company_tenure'] = df['company_tenure'].astype(int)\n",
    "df = df.drop(['Type of ownership'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bc5954",
   "metadata": {},
   "source": [
    "Data related to the industry and sector in which the company operates are available in the database. It has been decided to use the **Sector** variable for the study since the number of categories is smaller and it is easier to group. The same methodology that has been used for the **Type_ownership** is followed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = df['Sector'].value_counts()\n",
    "tabla = tabla.reset_index()\n",
    "tabla.columns = ['Sector', 'Frecuencia']\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ec0512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sector'] = df['Sector'].replace(['Biotech & Pharmaceuticals','Health Care'], 'Health')\n",
    "df['Sector'] = df['Sector'].replace(['Information Technology','Aerospace & Defense','Government','Telecommunications','Oil, Gas, Energy & Utilities','Agriculture & Forestry','Construction, Repair & Maintenance'], 'Engineering')\n",
    "df['Sector'] = df['Sector'].replace(['Consumer Services','Business Services','Finance',], 'Business')\n",
    "df['Sector'] = df['Sector'].replace(['Transportation & Logistics','Manufacturing'], 'Logistics')\n",
    "df['Sector'] = df['Sector'].replace(['Accounting & Legal','Real Estate','Government','Insurance'], 'Law')\n",
    "df['Sector'] = df['Sector'].replace(['-1','Non-Profit','Retail', 'Media','Travel & Tourism','Government','Education'], 'Other')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0ead1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tabla = df['Sector'].value_counts()\n",
    "tabla = tabla.reset_index()\n",
    "tabla.columns = ['Sector', 'Frecuencia']\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d8bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Sector', data=df,palette='crest')\n",
    "\n",
    "# añade etiquetas a los ejes y título a la gráfica\n",
    "plt.xlabel('Número de ofertas según el sector')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Sector de la empresa')\n",
    "\n",
    "# muestra la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f9d15",
   "metadata": {},
   "source": [
    "In order to choose a sector in which to work and the quality of the offers offered by Glasdoor, it is interesting to compare the scores given by users according to the sector. It can be seen that in most cases the average rating is similar, with a deviation depending on the sector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade4abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x = 'Sector', y = 'Rating', data = df,palette='crest')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be1ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ df['Sector'] =='Logistics', 'Sector'] = 0\n",
    "df.loc[df['Sector'] =='Law', 'Sector'] = 1\n",
    "df.loc[df['Sector'] =='Health', 'Sector']   = 2\n",
    "df.loc[ df['Sector'] =='Other', 'Sector'] = 3\n",
    "df.loc[ df['Sector'] =='Business', 'Sector'] = 4\n",
    "df.loc[ df['Sector'] =='Engineering', 'Sector'] = 5\n",
    "df['Sector'] = df['Sector'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d932dc3a",
   "metadata": {},
   "source": [
    "For the processing of locations, two columns are involved in the database, one with the location of the company and the other corresponding to the location of the boss to whom the worker answers. First of all, the **Location** column has been transformed using the geopy library. By defining the **get_coordinates** function, the coordinates of the city are obtained, which in turn have been broken down into latitude and longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dee675",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22a2e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabce014",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"my_app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52ec0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coordinates(city):\n",
    "    location = geolocator.geocode(city + \", USA\")\n",
    "    return (location.latitude, location.longitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc68bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Coordenadas'] = df['Location'].apply(get_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Latitud', 'Longitud']] = pd.DataFrame(df['Coordenadas'].tolist(), index=df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969d29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "frec_location = df['Location'].value_counts()\n",
    "categorias_comunes = frec_location.head(5).index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1891f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "datos_filtrados = df.loc[df['Location'].isin(categorias_comunes)]\n",
    "sns.countplot(x='Location', data=datos_filtrados,palette='crest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ec8bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "mapa = folium.Map(location=[39.8283, -98.5795], zoom_start=4)\n",
    "frecuencias = df['Location'].value_counts()\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    ciudad = row['Location']\n",
    "    latitud = row['Latitud']\n",
    "    longitud = row['Longitud']\n",
    "    ofertas = frecuencias\n",
    "\n",
    "    marcador = folium.Marker(location=[latitud, longitud], tooltip=f'{ofertas} ofertas en {ciudad}')\n",
    "    marcador.add_to(mapa)\n",
    "mapa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d0aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a42248a",
   "metadata": {},
   "source": [
    "As for **Headquearters**, a new binary column has been created so that it takes a value of 1 when the location of the head and the company coincide and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd765520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def location_comparer(df, col1, col2, new_col_name):\n",
    "    # crea una nueva columna con 0 por defecto\n",
    "    df[new_col_name] = 0\n",
    "    \n",
    "    # compara los valores de las dos columnas y asigna 1 a la nueva columna si son iguales\n",
    "    df.loc[df[col1] == df[col2], new_col_name] = 1\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5b673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = location_comparer(df, 'Location', 'Headquarters', 'Unique_location')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9d7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='Unique_location', data=df,palette='crest')\n",
    "\n",
    "# añade etiquetas a los ejes y título a la gráfica\n",
    "plt.xlabel('Coincidencia de localización')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Jefe y empleado desarrollan su trabajo en el mismo lugar')\n",
    "\n",
    "# muestra la gráfica\n",
    "plt.show()\n",
    "df = df.drop(['Headquarters'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983e47d0",
   "metadata": {},
   "source": [
    "The variables **Salary** and **Revenue** are expressed in ranges but in character type variable. Therefore, it is necessary to make transformations in the variables and also the ranges will be restructured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f129f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['min_salary']=0\n",
    "df['max_salary']=0\n",
    "df['avg_salary']=0\n",
    "for i in range(len(df)):\n",
    "    try:\n",
    "        df.loc[i,\"min_salary\"]=int(df['Salary Estimate'][i].split(\" \")[0].split(\"-\")[0].replace(\"$\",\"\").replace(\"K\",\"\"))\n",
    "        df.loc[i,\"max_salary\"]=int(df['Salary Estimate'][i].split(\" \")[0].split(\"-\")[1].replace(\"$\",\"\").replace(\"K\",\"\"))\n",
    "    except:\n",
    "        df.loc[i,\"min_salary\"]=int(df['Salary Estimate'][i].split(\"(E\")[0].split(\"-\")[0].replace(\"$\",\"\").replace(\"K\",\"\"))\n",
    "        df.loc[i,\"max_salary\"]=int(df['Salary Estimate'][i].split(\"(E\")[0].split(\"-\")[1].replace(\"$\",\"\").replace(\"K\",\"\"))\n",
    "    finally:\n",
    "        df.loc[i,\"Salary Estimate\"]=str(df.loc[i,\"min_salary\"])+\"-\"+str(df.loc[i,\"max_salary\"])\n",
    "        df.loc[i,\"avg_salary\"]=np.mean([df.loc[i,\"min_salary\"],df.loc[i,\"max_salary\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Salary Estimate'], axis=1)\n",
    "df = df.drop(['Revenue'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebaa160",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(x='avg_salary', data=df,palette='crest')\n",
    "\n",
    "# añade etiquetas a los ejes y título a la gráfica\n",
    "plt.xlabel('Coincidencia de localización')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Jefe y empleado desarrollan su trabajo en el mismo lugar')\n",
    "\n",
    "# muestra la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb62845",
   "metadata": {},
   "source": [
    "The variables that are not necessary for the modeling are eliminated and a copy of the dataset is made up to this point, which will be needed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9bf29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Location'], axis=1)\n",
    "df = df.drop(['Size_min'], axis=1)\n",
    "df = df.drop(['Size_max'], axis=1)\n",
    "df = df.drop(['Industry'], axis=1)\n",
    "df = df.drop(['Coordenadas'], axis=1)\n",
    "df = df.drop(['Founded'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffe44f3",
   "metadata": {},
   "source": [
    "### VISUALIZATION OF DISTRIBUTIONS, IDENTIFICATION OF OUTLIERS AND CORRELATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2baa9b7a",
   "metadata": {},
   "source": [
    "Once the transformation of the variables has been completed, a visualization of the quantitative variables is made to better understand the current state of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c86e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['min_salary'])\n",
    "sns.kdeplot(df['min_salary'])\n",
    "plt.show()\n",
    "\n",
    "sns.distplot(df['max_salary'])\n",
    "sns.kdeplot(df['max_salary'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0a829b",
   "metadata": {},
   "source": [
    "The existence of outliers is analyzed once the variables have been transformed. These will be eliminated even though, being so few, they will have no influence on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc9c6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "colsNumeros = ['min_salary','max_salary','avg_salary','Rating','Size']\n",
    "fig,ax=plt.subplots(2,3,figsize=(12,10))\n",
    "index=0\n",
    "ax=ax.flatten()\n",
    "for col in colsNumeros:\n",
    "    sns.boxplot(y=col, data=df, color='b', ax=ax[index],palette='crest')\n",
    "    index+=1\n",
    "plt.tight_layout(pad=0.5, w_pad=1, h_pad=5.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83634283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile(df, percentile_rank, column):\n",
    "    \n",
    "    # First, sort by ascending gdp, reset the indices\n",
    "    df = df.sort_values(by=column).reset_index()\n",
    "    \n",
    "    index = (len(df.index)-1) * percentile_rank / 100.0\n",
    "    index = int(index)\n",
    "    \n",
    "    return (df.at[index, column])\n",
    "def interquartile_range(df,column):\n",
    "    \n",
    "    p75 = get_percentile(df, 75,column)  # 75th percentile country and gdp\n",
    "    p25 = get_percentile(df, 25,column)  # 25th percentile country and gdp\n",
    "    iqr = p75 - p25  # Interquartile Range\n",
    "    return iqr\n",
    "def get_outliers(df,column,k=1.5):\n",
    "    \n",
    "    # Compute the 25th percentile, the 75th percentile and the IQR\n",
    "    p25 = get_percentile(df, 25,column)\n",
    "    p75 = get_percentile(df, 75,column)\n",
    "    iqr = interquartile_range(df,column)\n",
    "    \n",
    "    # \"Minimum non-outlier value\": 25th percentile - 1.5 * IQR\n",
    "    min_val = p25 - k*iqr\n",
    "    # \"Maximum non-outlier value\": 75th percentile + 1.5 * IQR\n",
    "    max_val = p75 + k*iqr\n",
    "    #print(min_val,max_val)\n",
    "    \n",
    "    outliers = df[(df[column] < min_val) | (df[column] > max_val)].index #añadi .index sobre el codigo de la clase anterior\n",
    "    return outliers\n",
    "  \n",
    "def detect_outliers(columns,df):\n",
    "    outlier_indices = []\n",
    "\n",
    "    for column in columns:        \n",
    "        outlier_indices.extend(get_outliers(df,column))\n",
    "        return outlier_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00a62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "df = df.drop(detect_outliers(colsNumeros,df),axis = 0).reset_index(drop = True)\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ff79e",
   "metadata": {},
   "source": [
    "Once the outliers have been eliminated, the correlation between the variables is analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e968e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9d85d9",
   "metadata": {},
   "source": [
    "Once the correlation matrix is analyzed, the minimum and maximum variables are considered to be eliminated since the information they collect is included in the avg_salary variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9198d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['min_salary'], axis=1)\n",
    "df = df.drop(['max_salary'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b842e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('datos_glassdoor.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5f1919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy() #se utilizara posteriormente para modelado"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bdb9d1",
   "metadata": {},
   "source": [
    "### MODELING AND ANALYSIS OF RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9426281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1, random_state=20)\n",
    "df = df.drop(['df_index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a680ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd57a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'avg_salary'\n",
    "df_original = df.copy(deep=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bcdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history_train(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c9570",
   "metadata": {},
   "source": [
    "#### MODEL A - REGRESSION MODEL WITH NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380b883",
   "metadata": {},
   "source": [
    "For the first proposed model, a basic structure is chosen, with the same number of neurons in each layer except for the last one, which must correspond to the result to be obtained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab6f1e7",
   "metadata": {},
   "source": [
    "Firstly, the variables are scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6c8208",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {}\n",
    "\n",
    "for col_name in df.columns:\n",
    "    if (df[col_name].dtype == 'int32') or (df[col_name].dtype == 'float64'):\n",
    "        print(col_name + ' ' + str(df[col_name].dtype))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler = scaler.fit(df[col_name].values.reshape(-1, 1))\n",
    "        df[col_name] = scaler.transform(df[col_name].values.reshape(-1, 1))\n",
    "        scaler_dict[col_name] = scaler\n",
    "scaler_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f1181ee",
   "metadata": {},
   "source": [
    "Because the number of observations is small compared to those frequently used for neural network modeling, it was decided that the size of the test set should be 20%, while the validation set should be 10%. The variation of these samples has a great influence on the results obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "val_size = 0.1\n",
    "epochs = 20\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a3553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df\n",
    "                                     , test_size = test_size, random_state=120)\n",
    "train_df, val_df = train_test_split(train_df\n",
    "                                    , test_size = val_size, random_state=120)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_features = np.array(train_df[train_df.columns.difference([target])])\n",
    "val_features = np.array(val_df[val_df.columns.difference([target])])\n",
    "test_features = np.array(test_df[test_df.columns.difference([target])])\n",
    "\n",
    "train_labels = np.array(train_df[[target]])\n",
    "val_labels = np.array(val_df[[target]])\n",
    "test_labels = np.array(test_df[[target]])\n",
    "\n",
    "input_len = train_features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c2732a",
   "metadata": {},
   "source": [
    "Before adjusting the parameters it is important to check that a simple neural network works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7247e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model():\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units = input_len\n",
    "                    , input_dim = input_len\n",
    "                    , kernel_initializer='normal'\n",
    "                    , activation='relu'))\n",
    "\n",
    "    model.add(Dense(1, activation='relu', kernel_initializer='normal'))\n",
    "    \n",
    "    # Compile model    \n",
    "    model.compile(optimizer = tf.keras.optimizers.RMSprop(0.01)\n",
    "                  , loss='mse' \n",
    "                  , metrics=['mae', 'mse'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = make_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bc35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_features,\n",
    "                    train_labels,\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = epochs,\n",
    "                    validation_data = (val_features, val_labels), \n",
    "                    verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31979dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_train(history, \"mse\")\n",
    "plot_history_train(history, \"loss\")\n",
    "plot_history_train(history, \"mae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b704dc",
   "metadata": {},
   "source": [
    "It is verified that the neural network works and therefore a network as dynamic as possible is defined in order to subsequently perform a grid search.\n",
    "\n",
    "\n",
    "The parameters for regression with neural networks are defined:\n",
    "\n",
    "- activation function of the last layer: linear, so it is decided to implement relu, \n",
    "- loss function should be mean squared error\n",
    "- chosen metrics are mse and mae. \n",
    "- number of neurons at the output must be 1 since the result of the model is a single variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f89717",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(dense_layers=1, dense_dropout=0.0, RMS=0.01, verbose=False, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    for i in range(0, dense_layers):\n",
    "        model.add(Dense(units=input_len, input_dim=input_len, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dropout(dense_dropout))\n",
    "\n",
    "    model.add(Dense(1, activation='relu', kernel_initializer='normal'))\n",
    "\n",
    "    # Compile model    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(RMS), loss='mse', metrics=['mae', 'mse'])\n",
    "    \n",
    "    if verbose:\n",
    "        print('dense_layers:', dense_layers)\n",
    "        print('RMS:', RMS)\n",
    "        print(model.summary())\n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19c9c33",
   "metadata": {},
   "source": [
    "A random seed has been added to the model definition function to ensure reproducibility of the results. Setting the seeds ensures that the same sequences are generated in each model run. This is essential to be able to reproduce the same results in different runs and to make accurate comparisons between different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bbd781",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(dense_layers=1, dense_dropout=0.0, RMS=0.01, verbose=False, seed=22):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=input_len, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "    # Add dense layers\n",
    "    for _ in range(dense_layers):\n",
    "        model.add(Dense(units=10, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dropout(dense_dropout))\n",
    "\n",
    "    model.add(Dense(1, activation='relu', kernel_initializer='normal'))\n",
    "\n",
    "    # Compile model    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(RMS), loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "    if verbose:\n",
    "        print('dense_layers:', dense_layers)\n",
    "        print('RMS:', RMS)\n",
    "        print(model.summary())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c9286",
   "metadata": {},
   "source": [
    "When looking for the hyperparameters of the neural network, it is important to start with those that will have a greater weight in the result obtained from the network.\n",
    "\n",
    "First of all, the number of **number of dense layers** will be determined.\n",
    "Dense layers are the simplest layers of neural networks. In these layers, neurons are connected to each neuron of the previous layer, so increasing the number of layers, increases the information transmission units, and the number of times the weights are updated.\n",
    "\n",
    "To try to determine the optimal number, different models are defined with fixed parameters in which different numbers of layers are used.\n",
    "\n",
    "For this purpose, a number of layers between 2 and 6 is used, in order not to increase the size of the network excessively. It is important to analyze both the metric value and the resulting graphs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837298ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [2,3,4,5,6]\n",
    "batch_size = [32]\n",
    "epochs = [10]\n",
    "dense_dropout = [0.0]\n",
    "RMS = [0.001]\n",
    "\n",
    "# make a list of dictionaries containing every possible \n",
    "# combination in the grid as a (smaller) dictionary \n",
    "import itertools\n",
    "\n",
    "param_grid = dict(dense_layers = dense_layers\n",
    "                    , RMS = RMS\n",
    "                    , batch_size = batch_size\n",
    "                    , epochs = epochs\n",
    "                    , dense_dropout=dense_dropout\n",
    "                 )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "param_grid = [dict(zip(keys, param_grid)) for param_grid in itertools.product(*values)]\n",
    "\n",
    "print('Proposed ' + str(len(param_grid)) + ' models')\n",
    "\n",
    "for j in range(0, len(param_grid)):\n",
    "    \n",
    "    dense_layers = param_grid[j].get(\"dense_layers\")\n",
    "    dense_dropout = param_grid[j].get(\"dense_dropout\")\n",
    "    RMS = param_grid[j].get(\"RMS\")\n",
    "    batch_size = param_grid[j].get(\"batch_size\")\n",
    "    epochs = param_grid[j].get(\"epochs\")\n",
    "    \n",
    "    model = make_model(dense_layers = dense_layers\n",
    "                   , RMS = RMS\n",
    "                   , verbose = 1)\n",
    "    \n",
    "    history = model.fit(train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (val_features, val_labels), \n",
    "                        verbose = 0)\n",
    "    \n",
    "    plot_history_train(history, 'mae')\n",
    "    mae = pd.DataFrame.from_dict(history.history)['val_mae'].iloc[-1]\n",
    "    param_grid[j].update( {\"val_mae\":mae})\n",
    "    \n",
    "results = pd.DataFrame(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5bea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['val_mae'],ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9f72c5",
   "metadata": {},
   "source": [
    "When the number of dense layers is 2, the model does not learn. The loss function is constant, so it is concluded that the weights are not being updated, they are always the same.\n",
    "\n",
    "Since the result obtained with 3 and 4 is practically the same, it is decided to use 3 dense layers, to avoid an unnecessary increase of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce5654",
   "metadata": {},
   "source": [
    "Although the optimal way to proceed is to continue with the study of those parameters that have a greater influence on the model, it can be seen from the previous graphs that the number of epochs is insufficient since the curves begin to descend but they lack the number of epochs sufficient to stabilize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bb15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [3]\n",
    "batch_size = [120]\n",
    "epochs = [20,40,60,80]\n",
    "dense_dropout = [0.0]\n",
    "RMS = [0.001]\n",
    "\n",
    "import itertools\n",
    "\n",
    "param_grid = dict(dense_layers = dense_layers\n",
    "                    , RMS = RMS\n",
    "                    , batch_size = batch_size\n",
    "                    , epochs = epochs\n",
    "                    , dense_dropout=dense_dropout\n",
    "                 )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "param_grid = [dict(zip(keys, param_grid)) for param_grid in itertools.product(*values)]\n",
    "\n",
    "print('Proposed ' + str(len(param_grid)) + ' models')\n",
    "\n",
    "for j in range(0, len(param_grid)):\n",
    "    \n",
    "    dense_layers = param_grid[j].get(\"dense_layers\")\n",
    "    dense_dropout = param_grid[j].get(\"dense_dropout\")\n",
    "    RMS = param_grid[j].get(\"RMS\")\n",
    "    batch_size = param_grid[j].get(\"batch_size\")\n",
    "    epochs = param_grid[j].get(\"epochs\")\n",
    "    \n",
    "    model = make_model(dense_layers = dense_layers\n",
    "                   , RMS = RMS\n",
    "                   , verbose = 1)\n",
    "    \n",
    "    history = model.fit(train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (val_features, val_labels), \n",
    "                        verbose = 0)\n",
    "    \n",
    "    plot_history_train(history, 'mae')\n",
    "    mae = pd.DataFrame.from_dict(history.history)['val_mae'].iloc[-1]\n",
    "    param_grid[j].update( {\"val_mae\":mae})\n",
    "    \n",
    "results = pd.DataFrame(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079384e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['val_mae'],ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93fdc40",
   "metadata": {},
   "source": [
    "When analyzing the graphs, it is observed that 20 epochs continue to be insufficient and it is when it is increased to 40 that the loss function has the necessary epochs to stabilize. For 60 and 80 the appearance and metrics are good, but it can be considered unnecessary, it is more favorable to stop earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0ed94c",
   "metadata": {},
   "source": [
    "Next, the batch size is modified, in which it is important to consider that if the number is small, the network has in memory a small amount of data, and it trains faster but it is possible that it does not learn the characteristics and details that can be significant in the prediction. On the other hand, if it is very large, the training will be slower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f186cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [3]\n",
    "batch_size = [16,32,64,128]\n",
    "epochs = [40]\n",
    "dense_dropout = [0.0]\n",
    "RMS = [0.001]\n",
    "\n",
    "# make a list of dictionaries containing every possible \n",
    "# combination in the grid as a (smaller) dictionary \n",
    "import itertools\n",
    "\n",
    "param_grid = dict(dense_layers = dense_layers\n",
    "                    , RMS = RMS\n",
    "                    , batch_size = batch_size\n",
    "                    , epochs = epochs\n",
    "                    , dense_dropout=dense_dropout\n",
    "                 )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "param_grid = [dict(zip(keys, param_grid)) for param_grid in itertools.product(*values)]\n",
    "\n",
    "print('Proposed ' + str(len(param_grid)) + ' models')\n",
    "\n",
    "for j in range(0, len(param_grid)):\n",
    "    \n",
    "    dense_layers = param_grid[j].get(\"dense_layers\")\n",
    "    dense_dropout = param_grid[j].get(\"dense_dropout\")\n",
    "    RMS = param_grid[j].get(\"RMS\")\n",
    "    batch_size = param_grid[j].get(\"batch_size\")\n",
    "    epochs = param_grid[j].get(\"epochs\")\n",
    "    \n",
    "    model = make_model(dense_layers = dense_layers\n",
    "                   , RMS = RMS\n",
    "                   , verbose = 1)\n",
    "    \n",
    "    history = model.fit(train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (val_features, val_labels), \n",
    "                        verbose = 0)\n",
    "    \n",
    "    plot_history_train(history, 'mae')\n",
    "    mae = pd.DataFrame.from_dict(history.history)['val_mae'].iloc[-1]\n",
    "    param_grid[j].update( {\"val_mae\":mae})\n",
    "    \n",
    "results = pd.DataFrame(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6cd2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['val_mae'],ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8743cd",
   "metadata": {},
   "source": [
    "By modifying the batch_size values, it is contemplated that depending on the batch size, the loss function takes more or less time to stabilize. This makes sense with the above mentioned feature identification capability for prediction. \n",
    "\n",
    "Therefore, the optimal number is defined to be 64."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48311ce",
   "metadata": {},
   "source": [
    "The final model is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4097caca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [3]\n",
    "batch_size = [64]\n",
    "epochs = [40]\n",
    "dense_dropout = [0.0]\n",
    "RMS = [0.001]\n",
    "\n",
    "# make a list of dictionaries containing every possible \n",
    "# combination in the grid as a (smaller) dictionary \n",
    "import itertools\n",
    "\n",
    "param_grid = dict(dense_layers = dense_layers\n",
    "                    , RMS = RMS\n",
    "                    , batch_size = batch_size\n",
    "                    , epochs = epochs\n",
    "                    , dense_dropout=dense_dropout\n",
    "                 )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "param_grid = [dict(zip(keys, param_grid)) for param_grid in itertools.product(*values)]\n",
    "\n",
    "print('Proposed ' + str(len(param_grid)) + ' models')\n",
    "\n",
    "for j in range(0, len(param_grid)):\n",
    "    \n",
    "    dense_layers = param_grid[j].get(\"dense_layers\")\n",
    "    dense_dropout = param_grid[j].get(\"dense_dropout\")\n",
    "    RMS = param_grid[j].get(\"RMS\")\n",
    "    batch_size = param_grid[j].get(\"batch_size\")\n",
    "    epochs = param_grid[j].get(\"epochs\")\n",
    "    \n",
    "    model = make_model(dense_layers = dense_layers\n",
    "                   , RMS = RMS\n",
    "                   , verbose = 1)\n",
    "    \n",
    "    history = model.fit(train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (val_features, val_labels), \n",
    "                        verbose = 0)\n",
    "    \n",
    "    plot_history_train(history, 'mae')\n",
    "    mae = pd.DataFrame.from_dict(history.history)['val_mae'].iloc[-1]\n",
    "    param_grid[j].update( {\"val_mae\":mae})\n",
    "    \n",
    "results = pd.DataFrame(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c3b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['val_mae'],ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddad3bd",
   "metadata": {},
   "source": [
    "Finally, the results obtained are checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2b5030",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_test_df = test_df.copy(deep = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_features)\n",
    "original_target = 'original_' + target\n",
    "pred_target = 'pred_' + target\n",
    "\n",
    "test_df[original_target] = scaler_dict[target].inverse_transform(test_df[target].values.reshape(-1, 1))\n",
    "test_df[pred_target] = scaler_dict[target].inverse_transform(preds)\n",
    "\n",
    "\n",
    "pred = 'pred_' + str(target)\n",
    "original = 'original_' + str(target)\n",
    "test_df[[original, pred]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c76633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['tot_mean_target'] = test_df[original_target].mean()\n",
    "mean_baseline_error = abs((test_df[original_target]-test_df['tot_mean_target'])).mean()\n",
    "std_baseline_error = abs((test_df[original_target]-test_df['tot_mean_target'])).std()\n",
    "\n",
    "print('mean baseline error: '+ str(mean_baseline_error))\n",
    "print('std baseline error: '+ str(std_baseline_error))\n",
    "print('_______________________________________________')\n",
    "mean_model_error = abs((test_df[original_target]-test_df[pred_target])).mean()\n",
    "std_model_error = abs((test_df[original_target]-test_df[pred_target])).std()\n",
    "\n",
    "print('mean model error: '+ str(mean_model_error))\n",
    "print('std model error: '+ str(std_model_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b6bd89",
   "metadata": {},
   "source": [
    "**MODEL B - OTHER ARCHITECTURE**\n",
    "\n",
    "A melon-like arrangement of the number of neurons is used for the development of this model. This refers to an architecture in which the hidden layers have a melon-like shape, i.e., they become wider and then gradually reduce in size. \n",
    "\n",
    "It has been decided to implement this network because of the advantages it presents:\n",
    "\n",
    "- Ability to capture complex features: it allows the neural network to have a large number of neurons in the intermediate layers, which gives it a greater ability to capture complex features and patterns in the data.\n",
    "\n",
    "- Hierarchical feature extraction: as the hidden layers become wider and then narrower, the neural network can learn features at different levels of abstraction. Initial layers can capture simpler and more local features, while later layers can learn more abstract and global features. \n",
    "\n",
    "- Increased generalization capability: can help avoid overfitting by gradually reducing the size of hidden layers. This limits the network's ability to memorize training data and encourages greater generalization to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9db26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(dense_layers=1, dense_dropout=0.0, RMS=0.01, verbose=False, seed=20):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=input_len, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "    # Add melon-shaped layers\n",
    "    for i in range(1, dense_layers + 1):\n",
    "        if i <= (dense_layers + 1) // 2:\n",
    "            units = i + 1\n",
    "        else:\n",
    "            units = dense_layers + 1 - i\n",
    "        model.add(Dense(units=units, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dropout(dense_dropout))\n",
    "\n",
    "    model.add(Dense(1, activation='relu', kernel_initializer='normal'))\n",
    "\n",
    "    # Compile model    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(RMS), loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "    if verbose:\n",
    "        print('dense_layers:', dense_layers)\n",
    "        print('RMS:', RMS)\n",
    "        print(model.summary())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a4fe7c",
   "metadata": {},
   "source": [
    "In this case, for the determination of the number of dense layers, higher numbers are used to make it possible to appreciate the aforementioned structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148e7c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [6,7,8,9,10,11]\n",
    "batch_size = [64]\n",
    "epochs = [40]\n",
    "dense_dropout = [0.0]\n",
    "RMS = [0.001]\n",
    "\n",
    "# make a list of dictionaries containing every possible \n",
    "# combination in the grid as a (smaller) dictionary \n",
    "import itertools\n",
    "\n",
    "param_grid = dict(dense_layers = dense_layers\n",
    "                    , RMS = RMS\n",
    "                    , batch_size = batch_size\n",
    "                    , epochs = epochs\n",
    "                    , dense_dropout=dense_dropout\n",
    "                 )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "param_grid = [dict(zip(keys, param_grid)) for param_grid in itertools.product(*values)]\n",
    "\n",
    "print('Proposed ' + str(len(param_grid)) + ' models')\n",
    "\n",
    "for j in range(0, len(param_grid)):\n",
    "    \n",
    "    dense_layers = param_grid[j].get(\"dense_layers\")\n",
    "    dense_dropout = param_grid[j].get(\"dense_dropout\")\n",
    "    RMS = param_grid[j].get(\"RMS\")\n",
    "    batch_size = param_grid[j].get(\"batch_size\")\n",
    "    epochs = param_grid[j].get(\"epochs\")\n",
    "    \n",
    "    model = make_model(dense_layers = dense_layers\n",
    "                   , RMS = RMS\n",
    "                   , verbose = 1)\n",
    "    \n",
    "    history = model.fit(train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (val_features, val_labels), \n",
    "                        verbose = 0)\n",
    "    \n",
    "    plot_history_train(history, 'mae')\n",
    "    mae = pd.DataFrame.from_dict(history.history)['val_mae'].iloc[-1]\n",
    "    param_grid[j].update( {\"val_mae\":mae})\n",
    "    \n",
    "results = pd.DataFrame(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['val_mae'],ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773496e8",
   "metadata": {},
   "source": [
    "In this case, all network architectures learn and the validation curve is above the training curve. In general, the number of epochs is not sufficient for training the network. It is decided to use a total of 10 dense layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28c41f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dense_layers = [10]\n",
    "batch_size = [120]\n",
    "epochs = [20,40,60,80]\n",
    "dense_dropout = [0.0]\n",
    "RMS = [0.001]\n",
    "\n",
    "import itertools\n",
    "\n",
    "param_grid = dict(dense_layers = dense_layers\n",
    "                    , RMS = RMS\n",
    "                    , batch_size = batch_size\n",
    "                    , epochs = epochs\n",
    "                    , dense_dropout=dense_dropout\n",
    "                 )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "param_grid = [dict(zip(keys, param_grid)) for param_grid in itertools.product(*values)]\n",
    "\n",
    "print('Proposed ' + str(len(param_grid)) + ' models')\n",
    "\n",
    "for j in range(0, len(param_grid)):\n",
    "    \n",
    "    dense_layers = param_grid[j].get(\"dense_layers\")\n",
    "    dense_dropout = param_grid[j].get(\"dense_dropout\")\n",
    "    RMS = param_grid[j].get(\"RMS\")\n",
    "    batch_size = param_grid[j].get(\"batch_size\")\n",
    "    epochs = param_grid[j].get(\"epochs\")\n",
    "    \n",
    "    model = make_model(dense_layers = dense_layers\n",
    "                   , RMS = RMS\n",
    "                   , verbose = 1)\n",
    "    \n",
    "    history = model.fit(train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (val_features, val_labels), \n",
    "                        verbose = 0)\n",
    "    \n",
    "    plot_history_train(history, 'mae')\n",
    "    mae = pd.DataFrame.from_dict(history.history)['val_mae'].iloc[-1]\n",
    "    param_grid[j].update( {\"val_mae\":mae})\n",
    "    \n",
    "results = pd.DataFrame(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bef4634",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['val_mae'],ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca00b1",
   "metadata": {},
   "source": [
    "When increasing the number of epochs, a behavior similar to that of the previous structure is observed, but in this case using 40 epochs could be risky for the model. In this case, 50 is defined as the optimum number of epochs.\n",
    "\n",
    "Finally, this architecture varies the value of the optimizer to analyze the effect it has on the results of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1cf11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [10]\n",
    "batch_size = [64]\n",
    "epochs = [50]\n",
    "dense_dropout = [0.0]\n",
    "RMS = [0.001,0.01,0.1]\n",
    "\n",
    "import itertools\n",
    "\n",
    "param_grid = dict(dense_layers = dense_layers\n",
    "                    , RMS = RMS\n",
    "                    , batch_size = batch_size\n",
    "                    , epochs = epochs\n",
    "                    , dense_dropout=dense_dropout\n",
    "                 )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "param_grid = [dict(zip(keys, param_grid)) for param_grid in itertools.product(*values)]\n",
    "\n",
    "print('Proposed ' + str(len(param_grid)) + ' models')\n",
    "\n",
    "for j in range(0, len(param_grid)):\n",
    "    \n",
    "    dense_layers = param_grid[j].get(\"dense_layers\")\n",
    "    dense_dropout = param_grid[j].get(\"dense_dropout\")\n",
    "    RMS = param_grid[j].get(\"RMS\")\n",
    "    batch_size = param_grid[j].get(\"batch_size\")\n",
    "    epochs = param_grid[j].get(\"epochs\")\n",
    "    \n",
    "    model = make_model(dense_layers = dense_layers\n",
    "                   , RMS = RMS\n",
    "                   , verbose = 1)\n",
    "    \n",
    "    history = model.fit(train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (val_features, val_labels), \n",
    "                        verbose = 0)\n",
    "    \n",
    "    plot_history_train(history, 'mae')\n",
    "    mae = pd.DataFrame.from_dict(history.history)['val_mae'].iloc[-1]\n",
    "    param_grid[j].update( {\"val_mae\":mae})\n",
    "    \n",
    "results = pd.DataFrame(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df6ff1e",
   "metadata": {},
   "source": [
    "It can be seen that when this value changes, the training does not really work for that test sample, so it would be necessary to perform a different division or obtain more samples to obtain consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac94d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['val_mae'],ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c538af",
   "metadata": {},
   "source": [
    "Check this out which takes dropout into account here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e30a171",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [10]\n",
    "batch_size = [64]\n",
    "epochs = [50]\n",
    "dense_dropout = [0.0,0.1,0.5]\n",
    "RMS = [0.001]\n",
    "\n",
    "# make a list of dictionaries containing every possible \n",
    "# combination in the grid as a (smaller) dictionary \n",
    "import itertools\n",
    "\n",
    "param_grid = dict(dense_layers = dense_layers\n",
    "                    , RMS = RMS\n",
    "                    , batch_size = batch_size\n",
    "                    , epochs = epochs\n",
    "                    , dense_dropout=dense_dropout\n",
    "                 )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "param_grid = [dict(zip(keys, param_grid)) for param_grid in itertools.product(*values)]\n",
    "\n",
    "print('Proposed ' + str(len(param_grid)) + ' models')\n",
    "\n",
    "for j in range(0, len(param_grid)):\n",
    "    \n",
    "    dense_layers = param_grid[j].get(\"dense_layers\")\n",
    "    dense_dropout = param_grid[j].get(\"dense_dropout\")\n",
    "    RMS = param_grid[j].get(\"RMS\")\n",
    "    batch_size = param_grid[j].get(\"batch_size\")\n",
    "    epochs = param_grid[j].get(\"epochs\")\n",
    "    \n",
    "    model = make_model(dense_layers = dense_layers\n",
    "                   , RMS = RMS,dense_dropout=dense_dropout\n",
    "                   , verbose = 1)\n",
    "    \n",
    "    history = model.fit(train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (val_features, val_labels), \n",
    "                        verbose = 0)\n",
    "    \n",
    "    plot_history_train(history, 'mae')\n",
    "    mae = pd.DataFrame.from_dict(history.history)['val_mae'].iloc[-1]\n",
    "    param_grid[j].update( {\"val_mae\":mae})\n",
    "    \n",
    "results = pd.DataFrame(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb6e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.sort_values(['val_mae'],ascending = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572d102c",
   "metadata": {},
   "source": [
    "**MODELO C - FEATURE ENGINEERING**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb8ffe7",
   "metadata": {},
   "source": [
    "Since the feature engineering work had been done previously due to the format of the available variables, in order to improve the accuracy of the network, the categorical variables are encoded using mean encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259766b5",
   "metadata": {},
   "source": [
    "Using this model as a basis, the defined variables are transformed into ranges with mean encoding. Instead of assigning a label or an arbitrary number to each category, Mean Encoding uses the information of the target variable to assign a numerical value to each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e3ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.groupby(['company_tenure'])['avg_salary'].mean())\n",
    "Mean_encoded_subject = df1.groupby(['company_tenure'])['avg_salary'].mean().to_dict()\n",
    "df1['company_tenure'] =  df1['company_tenure'].map(Mean_encoded_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e7617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.groupby(['Size'])['avg_salary'].mean())\n",
    "Mean_encoded_subject = df1.groupby(['Size'])['avg_salary'].mean().to_dict()\n",
    "df1['Size'] =  df1['Size'].map(Mean_encoded_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34efd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.groupby(['Sector'])['avg_salary'].mean())\n",
    "Mean_encoded_subject = df1.groupby(['Sector'])['avg_salary'].mean().to_dict()\n",
    "df1['Sector'] =  df1['Sector'].map(Mean_encoded_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7683cd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.groupby(['Type_ownership'])['avg_salary'].mean())\n",
    "Mean_encoded_subject = df1.groupby(['Type_ownership'])['avg_salary'].mean().to_dict()\n",
    "df1['Type_ownership'] =  df1['Type_ownership'].map(Mean_encoded_subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a90d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_dict = {}\n",
    "\n",
    "for col_name in df1.columns:\n",
    "    if (df1[col_name].dtype == 'int32') or (df1[col_name].dtype == 'float64'):\n",
    "        print(col_name + ' ' + str(df1[col_name].dtype))\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        scaler = scaler.fit(df1[col_name].values.reshape(-1, 1))\n",
    "        df1[col_name] = scaler.transform(df1[col_name].values.reshape(-1, 1))\n",
    "        scaler_dict[col_name] = scaler\n",
    "scaler_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a35aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.sample(frac=1)\n",
    "train_df, test_df = train_test_split(df1\n",
    "                                     , test_size = test_size, random_state=120)\n",
    "train_df, val_df = train_test_split(train_df\n",
    "                                    , test_size = val_size, random_state=120)\n",
    "\n",
    "# Form np arrays of labels and features.\n",
    "train_features = np.array(train_df[train_df.columns.difference([target])])\n",
    "val_features = np.array(val_df[val_df.columns.difference([target])])\n",
    "test_features = np.array(test_df[test_df.columns.difference([target])])\n",
    "\n",
    "train_labels = np.array(train_df[[target]])\n",
    "val_labels = np.array(val_df[[target]])\n",
    "test_labels = np.array(test_df[[target]])\n",
    "\n",
    "input_len = train_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff706fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(dense_layers=1, dense_dropout=0.0, RMS=0.01, verbose=False, seed=20):\n",
    "    os.environ['PYTHONHASHSEED'] = '0'\n",
    "    np.random.seed(seed)\n",
    "    rn.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    tf.compat.v1.set_random_seed(seed)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=10, input_dim=input_len, kernel_initializer='normal', activation='relu'))\n",
    "\n",
    "    # Add melon-shaped layers\n",
    "    for i in range(1, dense_layers + 1):\n",
    "        if i <= (dense_layers + 1) // 2:\n",
    "            units = i + 1\n",
    "        else:\n",
    "            units = dense_layers + 1 - i\n",
    "        model.add(Dense(units=units, kernel_initializer='normal', activation='relu'))\n",
    "        model.add(Dropout(dense_dropout))\n",
    "\n",
    "    model.add(Dense(1, activation='relu', kernel_initializer='normal'))\n",
    "\n",
    "    # Compile model    \n",
    "    model.compile(optimizer=tf.keras.optimizers.RMSprop(RMS), loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "    if verbose:\n",
    "        print('dense_layers:', dense_layers)\n",
    "        print('RMS:', RMS)\n",
    "        print(model.summary())\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef5cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_layers = [10]\n",
    "batch_size = [64]\n",
    "epochs = [50]\n",
    "dense_dropout = [0.0,0.1,0.5]\n",
    "RMS = [0.001]\n",
    "\n",
    "# make a list of dictionaries containing every possible \n",
    "# combination in the grid as a (smaller) dictionary \n",
    "import itertools\n",
    "\n",
    "param_grid = dict(dense_layers = dense_layers\n",
    "                    , RMS = RMS\n",
    "                    , batch_size = batch_size\n",
    "                    , epochs = epochs\n",
    "                    , dense_dropout=dense_dropout\n",
    "                 )\n",
    "\n",
    "keys = param_grid.keys()\n",
    "values = (param_grid[key] for key in keys)\n",
    "param_grid = [dict(zip(keys, param_grid)) for param_grid in itertools.product(*values)]\n",
    "\n",
    "print('Proposed ' + str(len(param_grid)) + ' models')\n",
    "\n",
    "for j in range(0, len(param_grid)):\n",
    "    \n",
    "    dense_layers = param_grid[j].get(\"dense_layers\")\n",
    "    dense_dropout = param_grid[j].get(\"dense_dropout\")\n",
    "    RMS = param_grid[j].get(\"RMS\")\n",
    "    batch_size = param_grid[j].get(\"batch_size\")\n",
    "    epochs = param_grid[j].get(\"epochs\")\n",
    "    \n",
    "    model = make_model(dense_layers = dense_layers\n",
    "                   , RMS = RMS,dense_dropout=dense_dropout\n",
    "                   , verbose = 1)\n",
    "    \n",
    "    history = model.fit(train_features,\n",
    "                        train_labels,\n",
    "                        batch_size = batch_size,\n",
    "                        epochs = epochs,\n",
    "                        validation_data = (val_features, val_labels), \n",
    "                        verbose = 0)\n",
    "    \n",
    "    plot_history_train(history, 'mae')\n",
    "    mae = pd.DataFrame.from_dict(history.history)['val_mae'].iloc[-1]\n",
    "    param_grid[j].update( {\"val_mae\":mae})\n",
    "    \n",
    "results = pd.DataFrame(param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a15da96",
   "metadata": {},
   "source": [
    "### CONCLUSIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d10854f",
   "metadata": {},
   "source": [
    "A neural network can be a useful tool for employers and job applicants by providing a deeper understanding of salary trends in a specific field or industry. Employers can use this technique to determine the competitiveness of their salary compared to other employers in the same field and applicants can use it to get an estimate of the salary to be received."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
